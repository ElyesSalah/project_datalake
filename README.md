### ğŸ“Œ **README Complet pour DÃ©ploiement et Utilisation du Projet Data Lake**  
ğŸš€ Ce **README** dÃ©taille **chaque Ã©tape** pour **installer, configurer et exÃ©cuter** la solution **Data Lake** basÃ©e sur **S3, MySQL, MongoDB, Airflow et FastAPI**.  

---

## ğŸ“Œ **1. PrÃ©-requis**  
Avant de commencer, assure-toi dâ€™avoir les applications suivantes installÃ©es :

âœ… **Docker & Docker Compose** ([Installation](https://docs.docker.com/get-docker/))  
âœ… **Python 3.8+** ([Installation](https://www.python.org/downloads/))  
âœ… **MongoDB** ([Installation](https://www.mongodb.com/try/download/community))  
âœ… **MySQL** ([Installation](https://dev.mysql.com/downloads/installer/))  
âœ… **Git** ([Installation](https://git-scm.com/downloads))  

---

## ğŸ“Œ **2. Cloner le projet et installer les dÃ©pendances**  
Clone le projet et installe les bibliothÃ¨ques nÃ©cessaires :

```bash
git clone https://github.com/ElyesSalah/project_datalake.git
cd projet_datalake
```

Avec un environnement virtuel Python :
```bash
python -m venv venv
source venv/bin/activate  # Sur macOS/Linux
venv\Scripts\activate  # Sur Windows
```

Puis, installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

---

## ğŸ“Œ **3. Lancer les services (MySQL, MongoDB, Airflow) avec Docker**
Le projet utilise **Docker Compose** pour dÃ©marrer les services automatiquement.

### ğŸ›  **DÃ©marrer MySQL, MongoDB et Airflow**
```bash
docker-compose up -d
```
ğŸ“Œ **Ce que cela fait :**  
âœ… Lance **PostgreSQL pour Airflow**  
âœ… DÃ©marre **Airflow (Scheduler, Webserver)**  
âœ… Active **MongoDB**  

---

## ğŸ“Œ **4. VÃ©rifier et configurer Airflow**
Une fois lancÃ©, ouvre **Airflow UI** et connecte-toi :

ğŸ‘‰ **[http://localhost:8080](http://localhost:8080)**  
ğŸ“Œ **Identifiants :**
- **Utilisateur** : `admin`
- **Mot de passe** : `admin`

VÃ©rifie que le DAG **`pipeline_datalake`** est bien listÃ©.

---

## ğŸ“Œ **5. Ajouter et exÃ©cuter un DAG Airflow**
Le pipeline dâ€™Airflow est dÃ©fini dans **`dags/pipeline.py`**. Il exÃ©cute trois tÃ¢ches :
1. **`ingest_data`** â†’ RÃ©cupÃ¨re les donnÃ©es brutes.
2. **`transform_data`** â†’ Nettoie et stocke dans MySQL.
3. **`load_gold_data`** â†’ Enrichit et stocke dans MongoDB.

### **Activer le DAG**
1. Va dans lâ€™interface Airflow.
2. Active **`pipeline_datalake`**.
3. Clique sur â–¶ï¸ **"Trigger DAG"** pour lâ€™exÃ©cuter.

---

## ğŸ“Œ **6. VÃ©rifier les bases de donnÃ©es**
### **ğŸ”¹ MySQL (Silver)**
AccÃ¨de Ã  **MySQL Workbench** ou via terminal :
```bash
mysql -u root -p
USE projet_datalake;
SELECT * FROM wine_data LIMIT 5;
```

### **ğŸ”¹ MongoDB (Gold)**
AccÃ¨de Ã  **MongoDB Compass** ou via terminal :
```bash
mongo
use wine_gold
db.wine_collection.find().limit(5)
```

---

## ğŸ“Œ **7. Lancer lâ€™API FastAPI**
FastAPI expose plusieurs endpoints pour rÃ©cupÃ©rer les donnÃ©es du **Data Lake**.

### **1ï¸âƒ£ DÃ©marrer l'API**
Dans le dossier du projet :
```bash
uvicorn api:app --host 0.0.0.0 --port 8000 --reload
```

### **2ï¸âƒ£ AccÃ©der aux donnÃ©es**
ğŸ‘‰ **Swagger UI** : [http://localhost:8000/docs](http://localhost:8000/docs)

ğŸ“Œ **Exemples de requÃªtes API :**
```bash
# Obtenir les donnÃ©es brutes depuis S3
curl http://localhost:8000/raw/winequality-red.csv

# Obtenir les donnÃ©es MySQL (Silver)
curl http://localhost:8000/staging?limit=5

# Obtenir les donnÃ©es enrichies (MongoDB)
curl http://localhost:8000/curated?limit=5
```

---

## ğŸ“Œ **8. Tester lâ€™ensemble du pipeline**
Des tests unitaires sont disponibles dans **`tests/`**.

### **1ï¸âƒ£ Tester les scripts indÃ©pendamment**
```bash
python scripts/01_ingest.py
python scripts/02_transform.py
python scripts/03_load_gold.py
```

### **2ï¸âƒ£ ExÃ©cuter les tests unitaires**
```bash
pytest tests/
```
ğŸ“Œ VÃ©rifie que tous les tests **passent avec succÃ¨s** âœ….

---

## ğŸ“Œ **9. DÃ©ploiement et utilisation en production**
Si tu veux **exÃ©cuter le projet en production**, voici quelques Ã©tapes recommandÃ©es :

### **1ï¸âƒ£ DÃ©marrer en mode production**
Utiliser `docker-compose.prod.yml` :
```bash
docker-compose -f docker-compose.prod.yml up -d
```

### **2ï¸âƒ£ DÃ©ployer lâ€™API sur un serveur**
- HÃ©bergement possible sur **AWS EC2, GCP, Azure**.
- Utilisation de **NGINX ou Traefik** pour exposer lâ€™API FastAPI.

---

## ğŸ“Œ **10. RÃ©sumÃ© des commandes essentielles**
| **Action** | **Commande** |
|------------|-------------|
| **1. Cloner le projet** | `git clone https://github.com/votre-repo/projet_datalake.git` |
| **2. Installer les dÃ©pendances** | `pip install -r requirements.txt` |
| **3. DÃ©marrer MySQL, MongoDB et Airflow** | `docker-compose up -d` |
| **4. AccÃ©der Ã  Airflow** | [http://localhost:8080](http://localhost:8080) |
| **5. Lancer lâ€™API FastAPI** | `uvicorn api:app --host 0.0.0.0 --port 8000 --reload` |
| **6. Tester les endpoints API** | `curl http://localhost:8000/wines` |
| **7. VÃ©rifier MySQL** | `mysql -u root -p` |
| **8. VÃ©rifier MongoDB** | `mongo wine_gold` |
| **9. ExÃ©cuter les tests unitaires** | `pytest tests/` |

---

ğŸš€ **Ton projet est maintenant bien structurÃ© et prÃªt Ã  Ãªtre exÃ©cutÃ© !**  
Dis-moi si tu veux ajouter quelque chose Ã  la documentation ! ğŸ˜Š
